{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "speech_to_text.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOq6+hL+2OuPApvso9f3+ek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michael-borck/worksheet-isys2001-s1-2023/blob/main/speech_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech to Text\n",
        "\n",
        "Part of your job is to produce a weekly summary (an intelligence report) of what is happening in your field. Sources of content include\n",
        "* Social Media\n",
        "* Webpages\n",
        "* YouTube\n",
        "* Podcasts\n",
        "* Blog posts\n",
        "* Reports\n",
        "* Articles\n",
        "\n",
        "I am sure you can think of many more. It would be great if we could write a program to create this report.\n",
        "\n",
        "There is a lot to do here. Gather the data, summarise the data and collate it into a document. How do we process video or a podcast? How can I get data from a webpage or a database? How can I summarise this data? Can I save the data as an excel spreadsheet or word document?\n",
        "\n",
        "A good strategy would be not to solve it all at once. Can we easily break it down into a few more straightforward problems? If we can solve and combine in the right way the simple issues, we should be able to solve the bigger problem. This approach of breaking a problem into subsystems is known as top-down design or programming. We look more closely at this approach in later modules.  \n",
        "\n",
        "Our general approach will be to take a source of content and convert that content into text. If needed, summarise the text and append it to our report. Then tidy up the information and submit it to our boss. Except for the last step, most of these steps can be done by a computer.\n",
        "\n",
        "Today, we will focus on how to convert an audio source into text. The audio source could be a podcast, a video or some other recording. We are going to convert speech to text. If you want to know more, have a look at [The Ultimate Guide to Speech Recognition With Python](https://realpython.com/python-speech-recognition/)\n",
        "\n",
        "## Tasks\n",
        "* Import into GitHub\n",
        "* Write a program to convert speech to text\n",
        "* regularly save to GitHub with a commit message"
      ],
      "metadata": {
        "id": "WTD7HkIDMXsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install SpeechRecognition\n",
        "!wget https://www.pacdv.com/sounds/voices/maybe-next-time.wav\n"
      ],
      "metadata": {
        "id": "MX0KZcvyOeJ_",
        "outputId": "66be1383-f0e5-4ab8-f6ea-f998cd2dc460",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-05 02:30:46--  https://www.pacdv.com/sounds/voices/maybe-next-time.wav\n",
            "Resolving www.pacdv.com (www.pacdv.com)... 192.232.218.187\n",
            "Connecting to www.pacdv.com (www.pacdv.com)|192.232.218.187|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 141336 (138K) [audio/x-wav]\n",
            "Saving to: ‘maybe-next-time.wav.1’\n",
            "\n",
            "maybe-next-time.wav 100%[===================>] 138.02K   348KB/s    in 0.4s    \n",
            "\n",
            "2023-04-05 02:30:47 (348 KB/s) - ‘maybe-next-time.wav.1’ saved [141336/141336]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "r = sr.Recognizer()\n",
        "\n",
        "audio_file = sr.AudioFile('/content/maybe-next-time.wav')\n",
        "\n",
        "with audio_file as source:\n",
        "    audio_text = r.record(source)\n",
        "\n",
        "text = r.recognize_google(audio_text)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "id": "jpmIwK03SKOr",
        "outputId": "62b4eda6-7a90-4950-d873-16f63c8de862",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maybe next\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open ('text.txt', 'w') as file:  \n",
        "    for line in text:  \n",
        "        file.writelines(line)  "
      ],
      "metadata": {
        "id": "Q3rgyLifOPmI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def summarize_text(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "    summary = ' '.join(sentences[:2])\n",
        "    return summary\n",
        "\n",
        "#text = \"The quick brown fox jumped over the lazy dog. The dog slept over the verandah.\"\n",
        "text = input(\"Please input a passage of text:\")\n",
        "output = summarize_text(text)\n",
        "print(\"Sure..... Here is the summary\")\n",
        "print(output)\n",
        "\n",
        "#print(summarize_text(text))"
      ],
      "metadata": {
        "id": "6Sf7e1JWR5eh",
        "outputId": "7fecfc51-01f9-44d5-9d1a-19634bda7a1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please input a passage of text:Ever feel like you don’t have enough time to read everything that you want to? What if you could run a routine that summarized documents for you, whether it’s your favorite news source, academic articles, or work-related documents?  Text summarization is a Natural Language Processing (NLP) task that summarizes the information in large texts for quicker consumption without losing vital information. Your favourite news aggregator (such as Google News) takes advantage of text summarization algorithms in order to provide you with information you need to know whether the article is relevant or not without having to click the link.   This tutorial will walk you through a simple text summarization task. We’ll use Abstractive Text Summarization and packages like newspeper2k and PyPDF2 to convert the text into a format that Python understands. Finally, we’ll use SPaCy to summarize the text with deep learning. Once you understand how text summarization works, you can also try doing the same with audio files that need to be first transcribed to text.  text summarization tutorial python  There are two main text summarization methods:  Extractive Text Summarization – attempts to identify significant sentences and then adds them to the summary, which will contain exact sentences from the original text. Abstractive Text Summarization – attempts to identify important sections, interpret the context and intelligently generate a summary. In this article, we’ll use the abstractive method on a news article. The process is straightforward:  Install a Python environment that contains all of the packages that you’ll need for the task. Import the text to be summarized. Build, test and run the routine to summarize the text. All of the code used in this article can be found on my GitLab repository. All set? Let’s go.  Step 1: Installing Text Summarization Python Environment To follow along with the code in this article, you can download and install our pre-built Text Summarization environment, which contains a version of Python 3.8 and the packages used in this post.  activestate python environment  In order to download this ready-to-use Python environment, you will need to create an ActiveState Platform account. Just use your GitHub credentials or your email address to register. Signing up is easy, and it unlocks the many benefits of the ActiveState Platform!  For Linux users: run the following to automatically download and install our CLI, the State Tool, along with the Text Summarization into a virtual environment:  sh <(curl -q https://platform.activestate.com/dl/cli/install.sh) --activate-default Pizza-Team/Text-Summarization Step 2 – Choose a Text Source for Abstractive Text Summarization The quality, type, and density of information conveyed via text varies from source to source. Textbooks tend to be low in density but high in quality, while academic articles are high in both quality and density. On the other hand, news articles can vary significantly from source to source.   Regardless of where the text comes from the goal here is to minimize the time you spend reading. Thus, we will build a tool that can easily be adapted to any number of sources.   For this example, we will use a news article on a recent global warming study from ScienceDaily as our text source. Feel free to use a different article.   To extract the text from the URL, we’ll use the newspaper3k package:  from newspaper import Article  url = 'https://www.sciencedaily.com/releases/2021/08/210811162816.htm' article = Article(url) article.download() article.parse() Now, we’ll download and parse the article to extract the relevant attributes. From here, we can view the article text:  article.text deep learning tutorial\n",
            "Sure..... Here is the summary\n",
            "Ever feel like you don’t have enough time to read everything that you want to? What if you could run a routine that summarized documents for you, whether it’s your favorite news source, academic articles, or work-related documents?  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install python-docx"
      ],
      "metadata": {
        "id": "CNXZlDB5Uu1A",
        "outputId": "f09734ae-cf45-43f3-b6b2-a6b96a4b0e03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/5.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "\n",
        "document = Document()\n",
        "document.add_heading('This is the headding', 0)\n",
        "p = document.add_paragraph(output)\n",
        "#p.add_run('bold').bold = True\n",
        "#p.add_run(' and some ')\n",
        "#p.add_run('italic.').italic = True\n",
        "\n",
        "document.save('example.docx')"
      ],
      "metadata": {
        "id": "LfYMAkfWTNT4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3aLeljMTU2A2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}