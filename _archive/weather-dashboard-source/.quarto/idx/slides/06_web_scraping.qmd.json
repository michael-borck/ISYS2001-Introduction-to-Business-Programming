{"title":"Mastering Web Scraping and APIs in Python","markdown":{"yaml":{"title":"Mastering Web Scraping and APIs in Python","subtitle":"Unlock the Power of Data Extraction and Integration","author":"Michael Borck","format":{"pptx":{"reference-doc":"../template.pptx"},"pdf":{"toc":true,"colorlinks":true},"docx":{"toc":true,"highlight-style":"github"},"html":{"toc":true,"toc-expand":2,"embed-resources":true}}},"headingText":"What is Web Scraping?","containsRefs":false,"markdown":"\n\n\nWeb scraping is an automated method used to extract large amounts of data from websites.\nIt involves fetching the web page and extracting specific information according to the user's needs.\nHow Does It Work?\n\nWeb scraping tools (like BeautifulSoup, Scrapy) parse the HTML of web pages.\nThey identify and extract the data elements based on HTML tags, attributes, or other patterns.\nCommon Uses of Web Scraping:\n\nWeb scraping is a powerful tool for extracting data from websites, enabling a wide range of applications from market analysis to sentiment analysis. Understanding its definition and common uses will help you identify potential use cases for your projects.\n\n# Data Collection for ....\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Research Data: Collecting data on climate patterns from weather websites for environmental research.\n- Use Case: Researchers can gather data from multiple sources to analyze trends and make predictions.\n\n- Price Monitoring: Scraping e-commerce sites to track product prices.\n- Use Case: Businesses and consumers can monitor price changes and market trends to make informed decisions.\n\n- Market Analysis: Extracting customer reviews and ratings from review websites.\n- Use Case: Companies can analyze feedback to improve products and services.\n\n- Content Aggregation: Collecting news articles from various news websites.\n- Use Case: News aggregators can compile articles from different sources to provide a comprehensive news feed.\n:::\n\n::: {.column width=\"50%\"}\n- Lead Generation Example: Scraping contact information from business directories.\n- Use Case: Sales and marketing teams can build lists of potential clients for outreach.\n\n- Job Listings: Extracting job postings from job boards.\n- Use Case: Job seekers can use aggregated job listings to find employment opportunities more efficiently.\n\n- Sentiment Analysis: Collecting social media posts or comments for sentiment analysis.\n- Use Case: Businesses can gauge public opinion and sentiment towards their brand or products.\n:::\n\n> Have you ever needed data from a website that wasn't available for download? How would web scraping help in that situation?\n\n::::\n\n\n\n# What is an API?\n\nAn Application Programming Interface (API) is a set of rules and protocols for building and interacting with software applications.\nAPIs define methods and data formats that applications use to communicate with each other.\n\nAPIs are fundamental to modern software development, enabling seamless integration and communication between different software systems. Understanding APIs' definition and common uses will help you leverage them effectively in your projects.\n\n# Key Characteristics of APIs:\n\n- Interoperability: APIs enable different software systems to work together.\n- Abstraction: APIs provide a layer of abstraction, allowing developers to use functionality without needing to understand the underlying code.\n- Reusability: APIs allow developers to reuse existing functionalities, speeding up development and promoting consistency.\n\n\n# Common Uses of APIs:\n\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n- Web APIs: Using the OpenWeatherMap API to get weather data.\n- Use Case: Integrating real-time weather updates into an application.\n\n- Social Media APIs: Twitter API, Facebook Graph API.\n- Use Case: Posting updates to social media platforms, retrieving user data, or accessing social media analytics.\n\n- Payment Gateway APIs: PayPal API, Stripe API.\n- Use Case: Facilitating online payments, managing transactions, and integrating payment solutions into e-commerce websites.\n:::\n\n::: {.column width=\"60%\"}\n- Maps and Geolocation APIs: Google Maps API, Mapbox API.\n- Use Case: Embedding maps into websites or apps, providing directions, and geocoding addresses.\n\n- Data Access APIs: Public APIs from government agencies or open data sources.\n- Use Case: Accessing datasets for research, analytics, or application development.\n\n- Communication APIs: Twilio API.\n- Use Case: Sending SMS, making phone calls, or managing video calls programmatically.\ncontents...\n:::\n\n> Can you think of any applications or services you use daily that might be powered by APIs?\n\n::::\n\n\n\n\n# Webscraping vs APIs\n\n  | Feature            | Web Scraping                           | APIs                                      |\n  |--------------------|----------------------------------------|-------------------------------------------|\n  | Method of Access   | Extracts HTML content from web pages   | Sends requests to a server for structured data |\n  | Data Structure     | Often unstructured or semi-structured  | Well-structured (JSON/XML)                |\n  | Reliability        | Less reliable, can break if HTML changes | More stable, with documented endpoints   |\n  | Ease of Use        | More complex setup, especially for dynamic content | Easier with documentation and SDKs  |\n\n> Can you think of a scenario where web scraping might be more beneficial than using an API, and vice versa?\n\n# Legal Aspects and Compliance:\n\n1. **Terms of Service:**\n   - **Adherence:** Always review and adhere to the terms of service (ToS) of the websites and APIs you are accessing. Violating ToS can lead to legal action.\n   - **Example:** Some sites explicitly prohibit scraping in their ToS.\n2. **Intellectual Property:**\n   - **Respect:** Do not scrape content that is protected by copyright without permission. This includes images, articles, and other copyrighted materials.\n   - **Fair Use:** Understand the limitations and rights under the fair use doctrine.\n3. **Data Protection Laws:**\n   - **Regulations:** Comply with data protection laws such as GDPR (General Data Protection Regulation) in the EU, and CCPA (California Consumer Privacy Act) in the USA.\n   - **User Rights:** Ensure that any personal data collected is handled according to the rights of the data subjects, including the right to access and delete their data.\n4. **APIs Compliance:**\n   - **Rate Limits:** Abide by the rate limits and usage policies set by API providers to avoid getting banned.\n   - **Attribution:** Provide proper attribution if required by the API provider.\n\n# Ethical Considerations:\n\n1. **Respect for Website Owners:**\n   - **Permission:** Always check if the website allows scraping. Look for a `robots.txt` file to understand the site's policy.\n   - **Respect Load:** Avoid placing excessive load on a website by making too many requests in a short period. Use delays and rate limiting.\n2. **Data Privacy:**\n   - **Sensitive Data:** Be cautious when scraping personal or sensitive information. Ensure compliance with data protection regulations.\n   - **User Consent:** If scraping user-generated content, consider the privacy and consent of the individuals who created the content.\n3. **Transparency:**\n   - **Disclosure:** Be transparent about your data collection methods if asked. Avoid deceptive practices like masking the scraper as a regular user.\n\n> Can you think of an example where web scraping could potentially be unethical or illegal? What steps would you take to ensure compliance?\n\n\n# Tools for Using APIs\n\n**What is the Requests Library?**\n\n - **Definition:** Requests is a simple and elegant HTTP library for Python, designed to make HTTP requests easy and more human-friendly.\n - **Installation:** Can be easily installed using `pip install requests`.\n\n**Key Features:**\n\n - **Ease of Use:** Simple syntax for making HTTP requests, such as GET, POST, PUT, DELETE, etc.\n - **Robustness:** Handles various complexities like sessions, cookies, and headers seamlessly.\n - **Extensibility:** Supports adding custom authentication and handling redirects.\n\n# Basic Example of Using Requests:**\n\n```python\nimport requests\n\nresponse = requests.get('https://api.openweathermap.org/data/2.5/weather?q=London&appid=YOUR_API_KEY')\nprint(response.status_code)  # Outputs: 200 (if successful)\nprint(response.text)  # Outputs the response content as a string\n```\n\n# JSON\n\nJSON (JavaScript Object Notation) is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate.\n\nCommonly used format for API responses because of its simplicity and readability.\n\n# Parsing JSON Responses:\n  \n```python\ndata = response.json()  # Converts response content to a Python dictionary\nprint(data)\n```\n\n# Extracting Specific Data:\n\n```python\n# Extracting temperature information from the JSON response\ntemperature = data['main']['temp']\nprint(f\"The temperature in London is {temperature}K\")\n```\n\n\n# Live Coding: **Introduction to BeautifulSoup:**\n\nBeautifulSoup is a Python library used for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data easily.\n\n```python\npip install beautifulsoup4\npip install requests\n```\n\n# Live Coding: **Setting Up the Environment:**\n\n- **Import Necessary Libraries:**\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\n```\n\n# Live Coding: **Fetching a Web Page:**\n\n```python\nurl = 'http://example.com'\nresponse = requests.get(url)\nhtml_content = response.content\n```\n\n# Live Coding: **Creating a BeautifulSoup Object:**\n\n```python\nsoup = BeautifulSoup(html_content, 'html.parser')\n```\n\n# Live Coding: **Understanding HTML Structure:**\n\n```html\n<html>\n<head>\n    <title>Example Domain</title>\n</head>\n<body>\n    <h1>Example Domain</h1>\n    <p>This domain is for use in illustrative examples in documents.</p>\n    <a href=\"https://www.iana.org/domains/example\">More information...</a>\n</body>\n</html>\n```\n\n# Live Coding: **Extracting Data:**\n   - **Finding Elements by Tag Name:**\n\n     ```python\n     title = soup.title\n     print(title.text)  # Outputs: Example Domain\n     ```\n\n   - **Finding Elements by CSS Class:**\n\n     ```python\n     paragraph = soup.find('p')\n     print(paragraph.text)  # Outputs: This domain is for use in illustrative examples in documents.\n     ```\n\n   - **Finding Elements by Attributes:**\n\n     ```python\n     link = soup.find('a', href=True)\n     print(link['href'])  # Outputs: https://www.iana.org/domains/example\n     ```\n\n# Live Coding: **Navigating the Parse Tree:**\n\n```python\nbody = soup.body\nprint(body.h1.text)  # Outputs: Example Domain\nprint(body.p.text)   # Outputs: This domain is for use in illustrative examples in documents.\n```\n\n> What challenges might you face when scraping data from more complex web pages?\n\n\n# Recap of Web Scraping\n\nWeb scraping is the automated process of extracting data from websites. It involves fetching the HTML of a web page and parsing it to extract the desired information.\n\n- **Requests Library:** Used to send HTTP requests to retrieve the HTML content of web pages.\n - **Key Functions:** `requests.get(url)`, handling response status codes, and accessing response content.\n- **BeautifulSoup Library:** Used to parse HTML and XML documents, making it easier to navigate and extract data.\n - **Key Functions:** `BeautifulSoup(html_content, 'html.parser')`, finding elements (`find`, `find_all`), and navigating the parse tree.\n\n- **Fetching Web Page Content:**\n ```python\n import requests\n response = requests.get('http://example.com')\n html_content = response.content\n ```\n- **Parsing HTML Content:**\n ```python\n from bs4 import BeautifulSoup\n soup = BeautifulSoup(html_content, 'html.parser')\n ```\n- **Extracting Data:**\n ```python\n title = soup.title.text\n print(title)  # Outputs: Example Domain\n ```\n\n# **Quick Quiz:**\n   - **Question 1:** What is the main purpose of the Requests library in web scraping?\n\n   - **Question 2:** How do you parse HTML content using BeautifulSoup?\n\n   - **Question 3:** What method would you use to find all paragraph (`<p>`) tags in a web page using BeautifulSoup?\n\n# **Quick Quiz:**\n   - **Question 1:** What is the main purpose of the Requests library in web scraping?\n     - **Answer:** To send HTTP requests and retrieve HTML content of web pages.\n   - **Question 2:** How do you parse HTML content using BeautifulSoup?\n     - **Answer:** By creating a BeautifulSoup object with the HTML content and specifying a parser (`html.parser`).\n   - **Question 3:** What method would you use to find all paragraph (`<p>`) tags in a web page using BeautifulSoup?\n     - **Answer:** `soup.find_all('p')`\n\n# Recap of Using APIs\n\nAn API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate with each other. APIs facilitate the exchange of data and functionality between systems.\n\n- **Requests Library:**\n - **Purpose:** Used to send HTTP requests to interact with APIs and retrieve data.\n - **Key Functions:** `requests.get(url)`, `requests.post(url, data)`, handling response status codes, and accessing response content.\n- **JSON Module:**\n - **Purpose:** Used to parse and manipulate JSON data, which is a common format for API responses.\n - **Key Functions:** `response.json()`, `json.loads()`, and `json.dumps()`.\n\n1. **Key Steps in Using APIs:**\n   - **Sending HTTP Requests:**\n\n     ```python\n     import requests\n     response = requests.get('https://api.openweathermap.org/data/2.5/weather?q=London&appid=YOUR_API_KEY')\n     print(response.status_code)  # Outputs: 200 (if successful)\n     print(response.text)  # Outputs the response content as a string\n     ```\n   - **Handling JSON Responses:**\n\n     ```python\n     data = response.json()  # Converts response content to a Python dictionary\n     print(data)\n     ```\n   - **Extracting Specific Data:**\n\n     ```python\n     temperature = data['main']['temp']\n     print(f\"The temperature in London is {temperature}K\")\n     ```\n\n# **Quick Quiz:**\n\n   - **Question 1:** What is the main purpose of the Requests library when working with APIs?\n\n   - **Question 2:** How do you parse a JSON response from an API using the Requests library?\n\n   - **Question 3:** How would you extract a specific value from a JSON response, such as the temperature in a weather API response?\n\n\n# **Quick Quiz:**\n   - **Question 1:** What is the main purpose of the Requests library when working with APIs?\n     - **Answer:** To send HTTP requests and retrieve data from APIs.\n   - **Question 2:** How do you parse a JSON response from an API using the Requests library?\n     - **Answer:** By using the `response.json()` method to convert the JSON response to a Python dictionary.\n   - **Question 3:** How would you extract a specific value from a JSON response, such as the temperature in a weather API response?\n     - **Answer:** By accessing the relevant key in the dictionary, e.g., `data['main']['temp']`.\n\n\n# Advanced Web Scraping: **Scrapy:**\n   - **What is Scrapy?**\n     - Scrapy is a powerful and fast web scraping and web crawling framework for Python.\n     - It is designed for large-scale web scraping projects, providing a robust set of features for extracting data from websites.\n   - **Key Features:**\n     - **Spider Framework:** Allows the creation of spiders that define how a site should be scraped, including following links and extracting data.\n     - **Item Pipeline:** Provides a way to process the extracted data, such as cleaning or storing it.\n     - **Built-in Support for Handling Requests and Responses:** Makes it easy to manage requests, handle responses, and follow links.\n\n# Scrapy Example:\n\n```python\nimport scrapy\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        'http://quotes.toscrape.com/page/1/',\n    ]\n\n    def parse(self, response):\n        for quote in response.css('div.quote'):\n            yield {\n                'text': quote.css('span.text::text').get(),\n                'author': quote.css('span small::text').get(),\n                'tags': quote.css('div.tags a.tag::text').getall(),\n            }\n```\n\n# Advanced Web Scraping: **Selenium:**\n\nSelenium is a web testing framework that can be used for automating browser actions. It is particularly useful for scraping dynamic content that requires interaction with the web page.\n\n   - **Key Features:**\n     - **Browser Automation:** Allows you to control a web browser programmatically, including clicking buttons, filling forms, and navigating pages.\n     - **Handling JavaScript:** Can execute JavaScript and wait for elements to load, making it ideal for scraping content that loads dynamically.\n\n# Selenium Example:\n```python\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome(executable_path='/path/to/chromedriver')\ndriver.get('http://quotes.toscrape.com/js/')\n\nquotes = driver.find_elements_by_class_name('quote')\nfor quote in quotes:\n    print(quote.text)\n\ndriver.quit()\n```\n\n# **Challenges of Dynamic Content:**\n\n - **JavaScript-Rendered Pages:** Some web pages load content dynamically using JavaScript, making it difficult to scrape with basic tools like Requests and BeautifulSoup.\n - **Asynchronous Loading:** Content might load at different times, requiring the scraper to wait for elements to be fully loaded.\n\n# **Solutions: **Using Selenium:**\nSelenium can wait for specific elements to appear before extracting data, ensuring that all dynamic content is fully loaded.\n  \n```python\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\ndriver.get('http://quotes.toscrape.com/js/')\ntry:\n    element = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.CLASS_NAME, \"quote\"))\n    )\n    quotes = driver.find_elements_by_class_name('quote')\n    for quote in quotes:\n        print(quote.text)\nfinally:\n    driver.quit()\n```\n\n# Solution: **Using Scrapy with Splash:**\nSplash is a headless browser designed for web scraping that can be integrated with Scrapy to render JavaScript content.\n\n```python\nimport scrapy\nfrom scrapy_splash import SplashRequest\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        'http://quotes.toscrape.com/js/',\n    ]\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield SplashRequest(url, self.parse, args={'wait': 1})\n\n    def parse(self, response):\n        for quote in response.css('div.quote'):\n            yield {\n                'text': quote.css('span.text::text').get(),\n                'author': quote.css('span small::text').get(),\n                'tags': quote.css('div.tags a.tag::text').getall(),\n            }\n```\n\n# When to use each:\n\nWhen would you use Scrapy versus Selenium for a web scraping project? What are the benefits and drawbacks of each?\n\nUse Scrapy for:\n\n- Large-scale web scraping operations with low power consumption and high speed requirements.\n- Scenarios where data pipeline and scalability are crucial.\n\nUse Selenium for:\n\n- Complex web scraping operations with JavaScript-heavy websites.\n- Scenarios where handling multiple data formats is necessary.\n\n\n# Advanced API Usage: **Working with OAuth for Authentication:**\n\nOAuth (Open Authorization) is an open standard for access delegation commonly used as a way to grant websites or applications limited access to a user's information without exposing passwords.\n\n   - **Purpose:** Ensures secure access to APIs by using tokens instead of passwords.\n\n1. **How OAuth Works:**\n   - **Process:**\n     - **Client Registration:** Register your application with the API provider to receive a client ID and client secret.\n     - **Authorization Request:** Redirect the user to the API provider's authorization server to grant permission.\n     - **Authorization Grant:** If the user approves, the authorization server provides an authorization grant to the client.\n     - **Access Token Request:** The client exchanges the authorization grant for an access token.\n     - **Accessing Resources:** Use the access token to access protected resources from the API.\n\n# oAuth Example \n\n```python\nimport requests\nfrom requests_oauthlib import OAuth2Session\n\n# Client credentials\nclient_id = 'YOUR_CLIENT_ID'\nclient_secret = 'YOUR_CLIENT_SECRET'\nredirect_uri = 'YOUR_REDIRECT_URI'\n\n# OAuth2 endpoints\nauthorization_base_url = 'https://api.provider.com/oauth/authorize'\ntoken_url = 'https://api.provider.com/oauth/token'\n\n# Step 1: User Authorization\noauth = OAuth2Session(client_id, redirect_uri=redirect_uri)\nauthorization_url, state = oauth.authorization_url(authorization_base_url)\nprint(f'Please go to {authorization_url} and authorize access.')\n\n# Step 2: Get the authorization verifier code from the callback url\nredirect_response = input('Paste the full redirect URL here:')\n\n# Step 3: Fetch the access token\ntoken = oauth.fetch_token(token_url, authorization_response=redirect_response, client_secret=client_secret)\nprint(f'Access token: {token}')\n\n# Step 4: Access protected resources\nresponse = oauth.get('https://api.provider.com/protected/resource')\nprint(response.json())\n```\n\n# **Rate Limiting and Pagination:**\nRate limiting is a technique used by API providers to control the amount of incoming requests from a user or application to prevent abuse and ensure fair usage.\n\n- **Handling Rate Limits:**\n  - **Check Headers:** Most APIs return rate limit information in response headers (e.g., `X-Rate-Limit-Remaining`, `Retry-After`).\n  - **Implement Delays:** Use sleep functions to pause the execution of your code when the limit is reached.\n\n\n# Rate Limit **Example:**\n\n```python\nimport time\n\nresponse = requests.get('https://api.example.com/data')\nif response.status_code == 429:  # Too Many Requests\n    retry_after = int(response.headers.get('Retry-After', 60))\n    print(f'Rate limit exceeded. Retrying after {retry_after} seconds.')\n    time.sleep(retry_after)\n    response = requests.get('https://api.example.com/data')\n```\n\n# **Pagination:**\n\nPagination is a technique used to divide a large set of results into manageable pages, making it easier to retrieve and process data.\n\n - **Handling Pagination:**\n   - **API Documentation:** Follow the API's documentation to understand how pagination is implemented (e.g., `page`, `limit`, `offset` parameters).\n   - **Iterate Through Pages:** Implement a loop to fetch all pages until no more data is available.\n\n# Pagination **Example:**\n\n```python\nall_data = []\npage = 1\nwhile True:\n    response = requests.get(f'https://api.example.com/data?page={page}&limit=100')\n    data = response.json()\n    if not data:\n        break\n    all_data.extend(data)\n    page += 1\nprint(f'Total records fetched: {len(all_data)}')\n```\n\n# Test your understanding?\n\nWhy is OAuth important for API security? How would you handle rate limits in your projects?\n\n\n# Test your understanding?\n\nOAuth is crucial for API security as it allows secure, delegated access to resources without exposing user credentials. By using OAuth, applications can request access via tokens with limited permissions, reducing the risk of over-privileged access. Tokens are short-lived and revocable, providing a dynamic and secure way to manage access.\n\nTo handle rate limits in projects, implement strategies that monitor and control API request frequency. Use exponential backoff for retries and maintain an internal counter to track requests. Employ techniques like the token bucket algorithm to ensure requests are spread out over time, adhering to API rate limits and preventing disruptions.","srcMarkdownNoYaml":"\n\n# What is Web Scraping?\n\nWeb scraping is an automated method used to extract large amounts of data from websites.\nIt involves fetching the web page and extracting specific information according to the user's needs.\nHow Does It Work?\n\nWeb scraping tools (like BeautifulSoup, Scrapy) parse the HTML of web pages.\nThey identify and extract the data elements based on HTML tags, attributes, or other patterns.\nCommon Uses of Web Scraping:\n\nWeb scraping is a powerful tool for extracting data from websites, enabling a wide range of applications from market analysis to sentiment analysis. Understanding its definition and common uses will help you identify potential use cases for your projects.\n\n# Data Collection for ....\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n- Research Data: Collecting data on climate patterns from weather websites for environmental research.\n- Use Case: Researchers can gather data from multiple sources to analyze trends and make predictions.\n\n- Price Monitoring: Scraping e-commerce sites to track product prices.\n- Use Case: Businesses and consumers can monitor price changes and market trends to make informed decisions.\n\n- Market Analysis: Extracting customer reviews and ratings from review websites.\n- Use Case: Companies can analyze feedback to improve products and services.\n\n- Content Aggregation: Collecting news articles from various news websites.\n- Use Case: News aggregators can compile articles from different sources to provide a comprehensive news feed.\n:::\n\n::: {.column width=\"50%\"}\n- Lead Generation Example: Scraping contact information from business directories.\n- Use Case: Sales and marketing teams can build lists of potential clients for outreach.\n\n- Job Listings: Extracting job postings from job boards.\n- Use Case: Job seekers can use aggregated job listings to find employment opportunities more efficiently.\n\n- Sentiment Analysis: Collecting social media posts or comments for sentiment analysis.\n- Use Case: Businesses can gauge public opinion and sentiment towards their brand or products.\n:::\n\n> Have you ever needed data from a website that wasn't available for download? How would web scraping help in that situation?\n\n::::\n\n\n\n# What is an API?\n\nAn Application Programming Interface (API) is a set of rules and protocols for building and interacting with software applications.\nAPIs define methods and data formats that applications use to communicate with each other.\n\nAPIs are fundamental to modern software development, enabling seamless integration and communication between different software systems. Understanding APIs' definition and common uses will help you leverage them effectively in your projects.\n\n# Key Characteristics of APIs:\n\n- Interoperability: APIs enable different software systems to work together.\n- Abstraction: APIs provide a layer of abstraction, allowing developers to use functionality without needing to understand the underlying code.\n- Reusability: APIs allow developers to reuse existing functionalities, speeding up development and promoting consistency.\n\n\n# Common Uses of APIs:\n\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n- Web APIs: Using the OpenWeatherMap API to get weather data.\n- Use Case: Integrating real-time weather updates into an application.\n\n- Social Media APIs: Twitter API, Facebook Graph API.\n- Use Case: Posting updates to social media platforms, retrieving user data, or accessing social media analytics.\n\n- Payment Gateway APIs: PayPal API, Stripe API.\n- Use Case: Facilitating online payments, managing transactions, and integrating payment solutions into e-commerce websites.\n:::\n\n::: {.column width=\"60%\"}\n- Maps and Geolocation APIs: Google Maps API, Mapbox API.\n- Use Case: Embedding maps into websites or apps, providing directions, and geocoding addresses.\n\n- Data Access APIs: Public APIs from government agencies or open data sources.\n- Use Case: Accessing datasets for research, analytics, or application development.\n\n- Communication APIs: Twilio API.\n- Use Case: Sending SMS, making phone calls, or managing video calls programmatically.\ncontents...\n:::\n\n> Can you think of any applications or services you use daily that might be powered by APIs?\n\n::::\n\n\n\n\n# Webscraping vs APIs\n\n  | Feature            | Web Scraping                           | APIs                                      |\n  |--------------------|----------------------------------------|-------------------------------------------|\n  | Method of Access   | Extracts HTML content from web pages   | Sends requests to a server for structured data |\n  | Data Structure     | Often unstructured or semi-structured  | Well-structured (JSON/XML)                |\n  | Reliability        | Less reliable, can break if HTML changes | More stable, with documented endpoints   |\n  | Ease of Use        | More complex setup, especially for dynamic content | Easier with documentation and SDKs  |\n\n> Can you think of a scenario where web scraping might be more beneficial than using an API, and vice versa?\n\n# Legal Aspects and Compliance:\n\n1. **Terms of Service:**\n   - **Adherence:** Always review and adhere to the terms of service (ToS) of the websites and APIs you are accessing. Violating ToS can lead to legal action.\n   - **Example:** Some sites explicitly prohibit scraping in their ToS.\n2. **Intellectual Property:**\n   - **Respect:** Do not scrape content that is protected by copyright without permission. This includes images, articles, and other copyrighted materials.\n   - **Fair Use:** Understand the limitations and rights under the fair use doctrine.\n3. **Data Protection Laws:**\n   - **Regulations:** Comply with data protection laws such as GDPR (General Data Protection Regulation) in the EU, and CCPA (California Consumer Privacy Act) in the USA.\n   - **User Rights:** Ensure that any personal data collected is handled according to the rights of the data subjects, including the right to access and delete their data.\n4. **APIs Compliance:**\n   - **Rate Limits:** Abide by the rate limits and usage policies set by API providers to avoid getting banned.\n   - **Attribution:** Provide proper attribution if required by the API provider.\n\n# Ethical Considerations:\n\n1. **Respect for Website Owners:**\n   - **Permission:** Always check if the website allows scraping. Look for a `robots.txt` file to understand the site's policy.\n   - **Respect Load:** Avoid placing excessive load on a website by making too many requests in a short period. Use delays and rate limiting.\n2. **Data Privacy:**\n   - **Sensitive Data:** Be cautious when scraping personal or sensitive information. Ensure compliance with data protection regulations.\n   - **User Consent:** If scraping user-generated content, consider the privacy and consent of the individuals who created the content.\n3. **Transparency:**\n   - **Disclosure:** Be transparent about your data collection methods if asked. Avoid deceptive practices like masking the scraper as a regular user.\n\n> Can you think of an example where web scraping could potentially be unethical or illegal? What steps would you take to ensure compliance?\n\n\n# Tools for Using APIs\n\n**What is the Requests Library?**\n\n - **Definition:** Requests is a simple and elegant HTTP library for Python, designed to make HTTP requests easy and more human-friendly.\n - **Installation:** Can be easily installed using `pip install requests`.\n\n**Key Features:**\n\n - **Ease of Use:** Simple syntax for making HTTP requests, such as GET, POST, PUT, DELETE, etc.\n - **Robustness:** Handles various complexities like sessions, cookies, and headers seamlessly.\n - **Extensibility:** Supports adding custom authentication and handling redirects.\n\n# Basic Example of Using Requests:**\n\n```python\nimport requests\n\nresponse = requests.get('https://api.openweathermap.org/data/2.5/weather?q=London&appid=YOUR_API_KEY')\nprint(response.status_code)  # Outputs: 200 (if successful)\nprint(response.text)  # Outputs the response content as a string\n```\n\n# JSON\n\nJSON (JavaScript Object Notation) is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate.\n\nCommonly used format for API responses because of its simplicity and readability.\n\n# Parsing JSON Responses:\n  \n```python\ndata = response.json()  # Converts response content to a Python dictionary\nprint(data)\n```\n\n# Extracting Specific Data:\n\n```python\n# Extracting temperature information from the JSON response\ntemperature = data['main']['temp']\nprint(f\"The temperature in London is {temperature}K\")\n```\n\n\n# Live Coding: **Introduction to BeautifulSoup:**\n\nBeautifulSoup is a Python library used for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data easily.\n\n```python\npip install beautifulsoup4\npip install requests\n```\n\n# Live Coding: **Setting Up the Environment:**\n\n- **Import Necessary Libraries:**\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\n```\n\n# Live Coding: **Fetching a Web Page:**\n\n```python\nurl = 'http://example.com'\nresponse = requests.get(url)\nhtml_content = response.content\n```\n\n# Live Coding: **Creating a BeautifulSoup Object:**\n\n```python\nsoup = BeautifulSoup(html_content, 'html.parser')\n```\n\n# Live Coding: **Understanding HTML Structure:**\n\n```html\n<html>\n<head>\n    <title>Example Domain</title>\n</head>\n<body>\n    <h1>Example Domain</h1>\n    <p>This domain is for use in illustrative examples in documents.</p>\n    <a href=\"https://www.iana.org/domains/example\">More information...</a>\n</body>\n</html>\n```\n\n# Live Coding: **Extracting Data:**\n   - **Finding Elements by Tag Name:**\n\n     ```python\n     title = soup.title\n     print(title.text)  # Outputs: Example Domain\n     ```\n\n   - **Finding Elements by CSS Class:**\n\n     ```python\n     paragraph = soup.find('p')\n     print(paragraph.text)  # Outputs: This domain is for use in illustrative examples in documents.\n     ```\n\n   - **Finding Elements by Attributes:**\n\n     ```python\n     link = soup.find('a', href=True)\n     print(link['href'])  # Outputs: https://www.iana.org/domains/example\n     ```\n\n# Live Coding: **Navigating the Parse Tree:**\n\n```python\nbody = soup.body\nprint(body.h1.text)  # Outputs: Example Domain\nprint(body.p.text)   # Outputs: This domain is for use in illustrative examples in documents.\n```\n\n> What challenges might you face when scraping data from more complex web pages?\n\n\n# Recap of Web Scraping\n\nWeb scraping is the automated process of extracting data from websites. It involves fetching the HTML of a web page and parsing it to extract the desired information.\n\n- **Requests Library:** Used to send HTTP requests to retrieve the HTML content of web pages.\n - **Key Functions:** `requests.get(url)`, handling response status codes, and accessing response content.\n- **BeautifulSoup Library:** Used to parse HTML and XML documents, making it easier to navigate and extract data.\n - **Key Functions:** `BeautifulSoup(html_content, 'html.parser')`, finding elements (`find`, `find_all`), and navigating the parse tree.\n\n- **Fetching Web Page Content:**\n ```python\n import requests\n response = requests.get('http://example.com')\n html_content = response.content\n ```\n- **Parsing HTML Content:**\n ```python\n from bs4 import BeautifulSoup\n soup = BeautifulSoup(html_content, 'html.parser')\n ```\n- **Extracting Data:**\n ```python\n title = soup.title.text\n print(title)  # Outputs: Example Domain\n ```\n\n# **Quick Quiz:**\n   - **Question 1:** What is the main purpose of the Requests library in web scraping?\n\n   - **Question 2:** How do you parse HTML content using BeautifulSoup?\n\n   - **Question 3:** What method would you use to find all paragraph (`<p>`) tags in a web page using BeautifulSoup?\n\n# **Quick Quiz:**\n   - **Question 1:** What is the main purpose of the Requests library in web scraping?\n     - **Answer:** To send HTTP requests and retrieve HTML content of web pages.\n   - **Question 2:** How do you parse HTML content using BeautifulSoup?\n     - **Answer:** By creating a BeautifulSoup object with the HTML content and specifying a parser (`html.parser`).\n   - **Question 3:** What method would you use to find all paragraph (`<p>`) tags in a web page using BeautifulSoup?\n     - **Answer:** `soup.find_all('p')`\n\n# Recap of Using APIs\n\nAn API (Application Programming Interface) is a set of rules and protocols that allows different software applications to communicate with each other. APIs facilitate the exchange of data and functionality between systems.\n\n- **Requests Library:**\n - **Purpose:** Used to send HTTP requests to interact with APIs and retrieve data.\n - **Key Functions:** `requests.get(url)`, `requests.post(url, data)`, handling response status codes, and accessing response content.\n- **JSON Module:**\n - **Purpose:** Used to parse and manipulate JSON data, which is a common format for API responses.\n - **Key Functions:** `response.json()`, `json.loads()`, and `json.dumps()`.\n\n1. **Key Steps in Using APIs:**\n   - **Sending HTTP Requests:**\n\n     ```python\n     import requests\n     response = requests.get('https://api.openweathermap.org/data/2.5/weather?q=London&appid=YOUR_API_KEY')\n     print(response.status_code)  # Outputs: 200 (if successful)\n     print(response.text)  # Outputs the response content as a string\n     ```\n   - **Handling JSON Responses:**\n\n     ```python\n     data = response.json()  # Converts response content to a Python dictionary\n     print(data)\n     ```\n   - **Extracting Specific Data:**\n\n     ```python\n     temperature = data['main']['temp']\n     print(f\"The temperature in London is {temperature}K\")\n     ```\n\n# **Quick Quiz:**\n\n   - **Question 1:** What is the main purpose of the Requests library when working with APIs?\n\n   - **Question 2:** How do you parse a JSON response from an API using the Requests library?\n\n   - **Question 3:** How would you extract a specific value from a JSON response, such as the temperature in a weather API response?\n\n\n# **Quick Quiz:**\n   - **Question 1:** What is the main purpose of the Requests library when working with APIs?\n     - **Answer:** To send HTTP requests and retrieve data from APIs.\n   - **Question 2:** How do you parse a JSON response from an API using the Requests library?\n     - **Answer:** By using the `response.json()` method to convert the JSON response to a Python dictionary.\n   - **Question 3:** How would you extract a specific value from a JSON response, such as the temperature in a weather API response?\n     - **Answer:** By accessing the relevant key in the dictionary, e.g., `data['main']['temp']`.\n\n\n# Advanced Web Scraping: **Scrapy:**\n   - **What is Scrapy?**\n     - Scrapy is a powerful and fast web scraping and web crawling framework for Python.\n     - It is designed for large-scale web scraping projects, providing a robust set of features for extracting data from websites.\n   - **Key Features:**\n     - **Spider Framework:** Allows the creation of spiders that define how a site should be scraped, including following links and extracting data.\n     - **Item Pipeline:** Provides a way to process the extracted data, such as cleaning or storing it.\n     - **Built-in Support for Handling Requests and Responses:** Makes it easy to manage requests, handle responses, and follow links.\n\n# Scrapy Example:\n\n```python\nimport scrapy\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        'http://quotes.toscrape.com/page/1/',\n    ]\n\n    def parse(self, response):\n        for quote in response.css('div.quote'):\n            yield {\n                'text': quote.css('span.text::text').get(),\n                'author': quote.css('span small::text').get(),\n                'tags': quote.css('div.tags a.tag::text').getall(),\n            }\n```\n\n# Advanced Web Scraping: **Selenium:**\n\nSelenium is a web testing framework that can be used for automating browser actions. It is particularly useful for scraping dynamic content that requires interaction with the web page.\n\n   - **Key Features:**\n     - **Browser Automation:** Allows you to control a web browser programmatically, including clicking buttons, filling forms, and navigating pages.\n     - **Handling JavaScript:** Can execute JavaScript and wait for elements to load, making it ideal for scraping content that loads dynamically.\n\n# Selenium Example:\n```python\nfrom selenium import webdriver\n\ndriver = webdriver.Chrome(executable_path='/path/to/chromedriver')\ndriver.get('http://quotes.toscrape.com/js/')\n\nquotes = driver.find_elements_by_class_name('quote')\nfor quote in quotes:\n    print(quote.text)\n\ndriver.quit()\n```\n\n# **Challenges of Dynamic Content:**\n\n - **JavaScript-Rendered Pages:** Some web pages load content dynamically using JavaScript, making it difficult to scrape with basic tools like Requests and BeautifulSoup.\n - **Asynchronous Loading:** Content might load at different times, requiring the scraper to wait for elements to be fully loaded.\n\n# **Solutions: **Using Selenium:**\nSelenium can wait for specific elements to appear before extracting data, ensuring that all dynamic content is fully loaded.\n  \n```python\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\ndriver.get('http://quotes.toscrape.com/js/')\ntry:\n    element = WebDriverWait(driver, 10).until(\n        EC.presence_of_element_located((By.CLASS_NAME, \"quote\"))\n    )\n    quotes = driver.find_elements_by_class_name('quote')\n    for quote in quotes:\n        print(quote.text)\nfinally:\n    driver.quit()\n```\n\n# Solution: **Using Scrapy with Splash:**\nSplash is a headless browser designed for web scraping that can be integrated with Scrapy to render JavaScript content.\n\n```python\nimport scrapy\nfrom scrapy_splash import SplashRequest\n\nclass QuotesSpider(scrapy.Spider):\n    name = \"quotes\"\n    start_urls = [\n        'http://quotes.toscrape.com/js/',\n    ]\n\n    def start_requests(self):\n        for url in self.start_urls:\n            yield SplashRequest(url, self.parse, args={'wait': 1})\n\n    def parse(self, response):\n        for quote in response.css('div.quote'):\n            yield {\n                'text': quote.css('span.text::text').get(),\n                'author': quote.css('span small::text').get(),\n                'tags': quote.css('div.tags a.tag::text').getall(),\n            }\n```\n\n# When to use each:\n\nWhen would you use Scrapy versus Selenium for a web scraping project? What are the benefits and drawbacks of each?\n\nUse Scrapy for:\n\n- Large-scale web scraping operations with low power consumption and high speed requirements.\n- Scenarios where data pipeline and scalability are crucial.\n\nUse Selenium for:\n\n- Complex web scraping operations with JavaScript-heavy websites.\n- Scenarios where handling multiple data formats is necessary.\n\n\n# Advanced API Usage: **Working with OAuth for Authentication:**\n\nOAuth (Open Authorization) is an open standard for access delegation commonly used as a way to grant websites or applications limited access to a user's information without exposing passwords.\n\n   - **Purpose:** Ensures secure access to APIs by using tokens instead of passwords.\n\n1. **How OAuth Works:**\n   - **Process:**\n     - **Client Registration:** Register your application with the API provider to receive a client ID and client secret.\n     - **Authorization Request:** Redirect the user to the API provider's authorization server to grant permission.\n     - **Authorization Grant:** If the user approves, the authorization server provides an authorization grant to the client.\n     - **Access Token Request:** The client exchanges the authorization grant for an access token.\n     - **Accessing Resources:** Use the access token to access protected resources from the API.\n\n# oAuth Example \n\n```python\nimport requests\nfrom requests_oauthlib import OAuth2Session\n\n# Client credentials\nclient_id = 'YOUR_CLIENT_ID'\nclient_secret = 'YOUR_CLIENT_SECRET'\nredirect_uri = 'YOUR_REDIRECT_URI'\n\n# OAuth2 endpoints\nauthorization_base_url = 'https://api.provider.com/oauth/authorize'\ntoken_url = 'https://api.provider.com/oauth/token'\n\n# Step 1: User Authorization\noauth = OAuth2Session(client_id, redirect_uri=redirect_uri)\nauthorization_url, state = oauth.authorization_url(authorization_base_url)\nprint(f'Please go to {authorization_url} and authorize access.')\n\n# Step 2: Get the authorization verifier code from the callback url\nredirect_response = input('Paste the full redirect URL here:')\n\n# Step 3: Fetch the access token\ntoken = oauth.fetch_token(token_url, authorization_response=redirect_response, client_secret=client_secret)\nprint(f'Access token: {token}')\n\n# Step 4: Access protected resources\nresponse = oauth.get('https://api.provider.com/protected/resource')\nprint(response.json())\n```\n\n# **Rate Limiting and Pagination:**\nRate limiting is a technique used by API providers to control the amount of incoming requests from a user or application to prevent abuse and ensure fair usage.\n\n- **Handling Rate Limits:**\n  - **Check Headers:** Most APIs return rate limit information in response headers (e.g., `X-Rate-Limit-Remaining`, `Retry-After`).\n  - **Implement Delays:** Use sleep functions to pause the execution of your code when the limit is reached.\n\n\n# Rate Limit **Example:**\n\n```python\nimport time\n\nresponse = requests.get('https://api.example.com/data')\nif response.status_code == 429:  # Too Many Requests\n    retry_after = int(response.headers.get('Retry-After', 60))\n    print(f'Rate limit exceeded. Retrying after {retry_after} seconds.')\n    time.sleep(retry_after)\n    response = requests.get('https://api.example.com/data')\n```\n\n# **Pagination:**\n\nPagination is a technique used to divide a large set of results into manageable pages, making it easier to retrieve and process data.\n\n - **Handling Pagination:**\n   - **API Documentation:** Follow the API's documentation to understand how pagination is implemented (e.g., `page`, `limit`, `offset` parameters).\n   - **Iterate Through Pages:** Implement a loop to fetch all pages until no more data is available.\n\n# Pagination **Example:**\n\n```python\nall_data = []\npage = 1\nwhile True:\n    response = requests.get(f'https://api.example.com/data?page={page}&limit=100')\n    data = response.json()\n    if not data:\n        break\n    all_data.extend(data)\n    page += 1\nprint(f'Total records fetched: {len(all_data)}')\n```\n\n# Test your understanding?\n\nWhy is OAuth important for API security? How would you handle rate limits in your projects?\n\n\n# Test your understanding?\n\nOAuth is crucial for API security as it allows secure, delegated access to resources without exposing user credentials. By using OAuth, applications can request access via tokens with limited permissions, reducing the risk of over-privileged access. Tokens are short-lived and revocable, providing a dynamic and secure way to manage access.\n\nTo handle rate limits in projects, implement strategies that monitor and control API request frequency. Use exponential backoff for retries and maintain an internal counter to track requests. Employ techniques like the token bucket algorithm to ensure requests are spread out over time, adhering to API rate limits and preventing disruptions."},"formats":{"pptx":{"identifier":{"display-name":"Powerpoint","target-format":"pptx","base-format":"pptx"},"execute":{"fig-width":11,"fig-height":5.5,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"0":"*.qmd","1":"!*.ipynb","keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":false,"output-ext":"pptx","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"page-width":10},"pandoc":{"default-image-extension":"png","to":"pptx","reference-doc":"../template.pptx","output-file":"06_web_scraping.pptx"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"title":"Mastering Web Scraping and APIs in Python","subtitle":"Unlock the Power of Data Extraction and Integration","author":"Michael Borck"}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"0":"*.qmd","1":"!*.ipynb","keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","toc":true,"output-file":"06_web_scraping.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"title":"Mastering Web Scraping and APIs in Python","subtitle":"Unlock the Power of Data Extraction and Integration","author":"Michael Borck","colorlinks":true},"extensions":{"book":{"selfContainedOutput":true}}},"docx":{"identifier":{"display-name":"MS Word","target-format":"docx","base-format":"docx"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"0":"*.qmd","1":"!*.ipynb","keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"docx","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"page-width":6.5},"pandoc":{"default-image-extension":"png","to":"docx","toc":true,"highlight-style":"github","output-file":"06_web_scraping.docx"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"title":"Mastering Web Scraping and APIs in Python","subtitle":"Unlock the Power of Data Extraction and Integration","author":"Michael Borck"},"extensions":{"book":{"selfContainedOutput":true}}},"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"0":"*.qmd","1":"!*.ipynb","keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"embed-resources":true,"output-file":"06_web_scraping.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","theme":"cosmo","title":"Mastering Web Scraping and APIs in Python","subtitle":"Unlock the Power of Data Extraction and Integration","author":"Michael Borck","toc-expand":2},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}