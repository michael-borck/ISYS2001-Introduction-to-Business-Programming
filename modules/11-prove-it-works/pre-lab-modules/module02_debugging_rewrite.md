---
title: "Debugging as Verification: Understanding What Went Wrong"
subtitle: "Investigating Code Behavior with Systematic Inquiry and AI Assistance"
author: "Michael Borck"
format: 
  pptx:
    reference-doc: curtin_template.pptx
  html:
    theme: 
      - cosmo
    toc: true
    toc-expand: 2
    code-fold: true
    embed-resources: true
    fig-cap-location: bottom
    css: module-styles.css
  pdf:
    toc: false
    colorlinks: true
    geometry:
      - top=30mm
      - left=20mm
  docx:
    highlight-style: github
    toc: false
---

# Debugging: Verifying What Went Wrong

* **Bug found** = verification failed
* **Debugging** = investigating why verification failed
* **Your role:** Detective, not mechanic
* **AI's role:** Research assistant, not auto-fix tool

::: {.notes}
Welcome to Module 2. If testing is about verifying code works correctly, debugging is about investigating when that verification fails. A bug is simply evidence that your verification found a problem - now you need to understand why.

Think of yourself as a detective investigating a crime scene, not a mechanic randomly swapping parts until something works. Detectives gather evidence, form hypotheses, test theories, and reach conclusions. That's exactly what effective debugging looks like.

AI can be an incredibly helpful research assistant in your investigation - explaining error messages, suggesting where to look, helping you understand complex code behaviour. But AI shouldn't be your auto-fix button. Understanding why something broke is far more valuable than just making the error message disappear.

This module teaches you systematic investigation skills that work whether you're debugging your own code, code written by someone else, or code generated by AI.
:::

# What Makes Debugging Different with AI?

**Traditional debugging:**
- You wrote the code, you debug it
- You understand the intent

**AI-era debugging:**
- You specified behaviour, AI implemented it
- Code might work in unexpected ways
- **You verify the behaviour, not the implementation**

::: {.notes}
Debugging when working with AI has some unique characteristics. In traditional programming, you wrote every line, so you have intimate knowledge of how the code works. You can debug by recalling what you were thinking when you wrote it.

With AI-generated code, you specified what should happen, but the implementation might surprise you. AI might solve the problem in a way you didn't expect. This means you can't rely on implementation knowledge - you must debug based on behaviour.

This is actually excellent practice. Professional developers often debug code written by others, or code they wrote months ago and no longer remember. Debugging based on observable behaviour rather than assumed implementation is a more robust skill.

The key shift: don't ask "why did I write it this way?" Ask instead "what is this code actually doing?" Your verification tests define correct behaviour - debugging is about understanding why actual behaviour doesn't match expected behaviour.
:::

# The Core Debugging Questions

1. **What did I expect to happen?** (Your specification)
2. **What actually happened?** (The observed behaviour)
3. **Where do these diverge?** (Locating the problem)
4. **Why does it diverge there?** (Understanding the cause)
5. **What's the simplest fix?** (Resolution)

::: {.notes}
Every debugging session should follow this pattern of inquiry. These five questions structure your investigation.

First, clarify your expectations. What should have happened? This comes from your specifications, your test cases, your understanding of requirements. Be specific: "The function should return 5 when given inputs 2 and 3."

Second, observe what actually happened. "It returned 6 instead." Or "It crashed with an error." Or "It returned nothing." Precise observation is critical.

Third, locate where expectation and reality diverge. This is often the hardest part. The error might appear in one place but be caused by something earlier in the code flow.

Fourth, understand why. This is where you move from symptom to root cause. Not "the variable is None" but "the variable is None because the function returned None because the input validation rejected the input because..."

Fifth, fix it - and preferably, fix it simply. The goal isn't clever solutions, it's clear, correct code.

These questions work whether you're debugging a one-line function or a complex system, whether you wrote it or AI generated it.
:::

# Understanding Error Messages

```python
Traceback (most recent call last):
  File "calculator.py", line 15, in <module>
    result = divide(10, 0)
  File "calculator.py", line 8, in divide
    return a / b
ZeroDivisionError: division by zero
```

**Read from bottom to top:**
- What happened? (Division by zero)
- Where? (Line 8, in divide function)
- How did we get there? (Called from line 15)

::: {.notes}
Error messages are your first and most important clue. Python's error messages are actually quite informative if you know how to read them.

A traceback shows you the chain of function calls that led to the error. Read it from bottom to top. The bottom tells you what actually went wrong - in this case, "ZeroDivisionError: division by zero." That's the what.

Above that, you see where it happened: "File calculator.py, line 8, in divide" and the actual line of code: "return a / b". That's the where.

Further up, you see how you got there: "File calculator.py, line 15" called the divide function with arguments (10, 0). That's the path to the problem.

This is gold for debugging. You know what broke, where it broke, and how the program got there. But you still need to understand why - why was the second argument 0? Was it supposed to be? Should there be validation?

When you encounter an error you don't understand, this is perfect for AI assistance: "I got this error message. Can you explain in simple terms what it means and why it might happen?" But don't stop there - ask yourself the why questions too.
:::

# AI as Your Error Interpreter

**Instead of asking AI to fix:**
```
"Fix this error [paste code]"
```

**Ask AI to explain:**
```
"I'm getting this error: [error message]
In this code: [relevant code]
What does this error mean in simple terms?
What are common reasons this happens?"
```

**Then YOU decide the fix.**

::: {.notes}
Here's a crucial distinction in how to use AI for debugging. The temptation is to paste your error and code and say "fix this." Resist that temptation.

Instead, use AI to understand the error. Ask for explanation: "What does IndexError mean? Why would I get 'list index out of range'?" Ask about common causes: "What are typical reasons this error happens?"

AI is excellent at explaining technical concepts and providing context. It can tell you that an IndexError means you tried to access a position in a list that doesn't exist - like asking for the 10th item in a 5-item list.

But you should be the one who decides how to fix it. Look at your code. Based on the explanation, can you see why the index is out of range? Is your loop condition wrong? Are you starting from 1 instead of 0? Is the list empty when you expected it to have items?

Understanding the error empowers you to fix not just this instance, but to recognize and prevent similar errors in the future. Quick AI fixes rob you of that learning opportunity.

Use AI to explain, use your brain to fix.
:::

# The Power of Print Statements

```python
def calculate_total(prices, tax_rate):
    print(f"Input prices: {prices}")
    print(f"Input tax_rate: {tax_rate}")
    
    subtotal = sum(prices)
    print(f"Subtotal: {subtotal}")
    
    tax = subtotal * tax_rate
    print(f"Tax: {tax}")
    
    total = subtotal + tax
    print(f"Final total: {total}")
    
    return total
```

**Strategic prints show you what's actually happening inside.**

::: {.notes}
The humble print statement is your most powerful debugging tool. It's simple, it's universal, and it gives you a window into what's actually happening as your code runs.

Look at this example. We're not just printing the final answer - we're printing at each step of the calculation. This lets us verify our assumptions. Is the tax_rate what we expected? Did sum() work correctly? Is the tax calculation right?

When debugging, add print statements strategically:
- At function entry points (print the inputs)
- Before and after calculations (print intermediate results)
- Inside loops (print the iteration variable and current state)
- Before return statements (print what you're about to return)

This creates a trail of evidence. You can see exactly what values exist at each point. Often, you'll immediately spot "Oh, the tax_rate is 20 instead of 0.20" or "the prices list is empty."

Print debugging is not primitive or unprofessional - it's fast, effective, and requires no special tools. Even experienced developers use it constantly.

When using AI-generated code, prints help you understand what the code is actually doing versus what you thought it was doing.
:::

# Asking AI for Strategic Print Placement

**Effective prompt:**
```
"I have this function [paste function]
that should [describe expected behavior]
but it's giving wrong results.

Where should I add print statements to trace
what's happening? I want to see intermediate
values to find where it goes wrong."
```

**AI can suggest inspection points you might miss.**

::: {.notes}
Here's a smart way to use AI in debugging: ask it where to place print statements to investigate your problem.

AI can analyze code flow and suggest strategic places to inspect values. It might notice a variable that gets modified in multiple places, or a calculation that depends on several inputs, or a loop that might have unexpected behavior.

The prompt structure matters. Tell AI what you have, what you expected, and what's wrong. Then ask for investigation strategy, not fixes.

AI might suggest: "Add a print before the loop to see the initial list state, add one inside the loop to see how the accumulator changes each iteration, and add one after to see the final result."

This teaches you systematic investigation. Over time, you'll internalize these patterns and know where to look without AI's help. But when you're stuck, AI can suggest inspection strategies you might not have considered.

Remember: AI suggests where to look, you interpret what you find.
:::

# Reading Code to Understand Behavior

**When debugging AI-generated code:**

1. **Ignore implementation cleverness**
2. **Trace the actual logic flow**
3. **Ask: "What does this line actually do?"**
4. **Use print statements to verify your understanding**

::: {.notes}
A critical debugging skill is reading code to understand what it actually does, not what you think it should do. This is especially important with AI-generated code.

Don't be impressed or intimidated by clever implementations. Focus on behavior. Take it line by line: "This line creates an empty list. This line loops through the input. For each item, this line checks if... and if true, adds to the list. After the loop, this returns the list."

When you don't understand a line, this is perfect for AI: "What does this line do? [paste line with context] Explain it in simple terms."

But don't just read - verify your understanding. Add print statements: "If this variable should contain the filtered items at this point, let me print it and check."

Often, the bug is in your mental model of the code, not the code itself. You assumed variable X held one thing, but it actually holds another. Print statements reveal reality.

Reading code carefully, line by line, forcing yourself to understand each operation, is fundamental. The complexity doesn't matter - break it down to simple operations.
:::

# Using AI to Explain Complex Code

**Effective prompt:**
```
"Can you explain this code section in simple terms?
[paste code]

Specifically:
- What does each line do?
- What does this variable contain at each step?
- What's the overall logic?

Explain like I'm a beginner who knows basic Python."
```

::: {.notes}
When you encounter code you don't understand - whether you wrote it long ago or AI generated it - ask for explanation before asking for fixes.

The key is requesting simple explanations. Tell AI you're a beginner. Ask for line-by-line breakdown. Request that it explain what variables contain, not just what operations are performed.

Good AI explanations sound like: "Line 3 creates an empty dictionary to store results. Line 4 starts a loop through each item in the input list. Inside the loop, line 5 checks if the item is already a key in our dictionary..."

This helps you build a mental model of the code's behavior. Once you understand what it's doing, you can identify where that differs from what it should be doing.

Asking for explanation also helps you learn. Today you need AI to explain list comprehensions, next week you'll recognize them yourself. Today you need help with dictionary methods, next month they'll be familiar.

Use AI as a tutor for understanding code, not just a fixer for broken code.
:::

# Hypothesis-Driven Debugging

1. **Observe the symptom** (What's wrong?)
2. **Form a hypothesis** (Why might this happen?)
3. **Design a test** (How can I check if I'm right?)
4. **Run the test** (Add prints, check values)
5. **Conclude** (Was my hypothesis correct?)

**Iterate until you find the root cause.**

::: {.notes}
Professional debugging is hypothesis-driven. You don't randomly change things hoping something works - you form theories and test them systematically.

Start with observation: "The function returns None instead of a number." That's your symptom.

Form a hypothesis: "Maybe the calculation is happening but the return statement is in the wrong place." Or "Maybe the input validation is rejecting valid input." Or "Maybe there's a logic error in the calculation."

Design a test for your hypothesis. If you think the calculation isn't happening, add a print statement right before it. If you think the return is wrong, print what you're about to return.

Run the test. Execute the code and look at your print statements. Do they support or refute your hypothesis?

Conclude. If prints show the calculation is happening and is correct, but the return is still None, your hypothesis about the calculation was wrong. Form a new hypothesis.

This scientific method approach is far more effective than random trial and error. It's also faster - each test narrows down the possibilities.

AI can help generate hypotheses: "Given this error and this code, what are three possible causes?" But you should test those hypotheses yourself.
:::

# AI for Hypothesis Generation

**Effective prompt:**
```
"This function should [expected behavior]
but it actually [observed behavior]

Given this code: [paste code]

What are three possible reasons this might happen?
Don't give me fixes - just help me understand
what could cause this."
```

::: {.notes}
Here's another excellent use of AI in debugging: generating hypotheses about what might be wrong.

When you're stuck and can't figure out why code isn't working, describe the expected versus actual behavior and ask AI for possible causes. Not fixes - causes.

AI might suggest: "This could happen if: (1) the input is a different type than expected, (2) the calculation is using integer division instead of float division, or (3) the return statement is inside the conditional instead of after it."

These hypotheses give you specific things to check. You can add print statements to verify the input type, check what kind of division is being used, examine the indentation of the return statement.

This is collaborative debugging. AI provides theories based on pattern recognition from vast amounts of code. You test those theories against your specific situation.

Often, just seeing the list of possibilities helps you recognize which is most likely. "Oh, integer division - that would explain why I'm getting 2 instead of 2.5!"

Use AI to expand your thinking about what might be wrong, then use your own investigation to determine what actually is wrong.
:::

# Common Beginner Bug Patterns

**Off-by-one errors:**
```python
for i in range(len(items)):
    print(items[i+1])  # Crashes on last item!
```

**Wrong variable:**
```python
total = calculate_price(quantity, price)
print(totla)  # Typo - wrong variable
```

**Scope confusion:**
```python
if condition:
    result = calculate()
print(result)  # May not exist if condition false
```

::: {.notes}
Certain bug patterns appear repeatedly in beginner code. Learning to recognize these can speed up debugging dramatically.

Off-by-one errors are classic. You're looping through a list but your index goes one too far, or starts one too early. Print the index and the length to verify your loop bounds.

Typos in variable names are surprisingly common. Python will tell you "name 'totla' is not defined" but you might not immediately see it's a typo of 'total'. Read error messages carefully.

Scope issues - using a variable that might not exist depending on code paths. If result is only assigned inside an if block, but you use it outside, it might not exist. Print statements inside each branch help you see which path is actually executing.

When debugging, ask yourself: "Is this an off-by-one? Is this a typo? Is this a scope issue?" Check these common patterns first before diving into complex theories.

AI can help identify these: "Is this an off-by-one error?" But learning to spot them yourself makes you faster.
:::

# When to Accept vs Simplify AI's Fix

**If AI suggests a fix:**

1. **Understand the fix** - don't just copy it
2. **Verify it solves the root cause** - not just the symptom
3. **Check if it's too complex** - ask for simpler version
4. **Test it properly** - run your verification tests

**Red flags:**
- Fix uses features you haven't learned
- Fix adds complexity without clear benefit
- You don't understand why it works

::: {.notes}
When AI suggests a fix for your bug, don't immediately apply it. Evaluate it carefully.

First, understand what the fix does and why it solves the problem. If AI changes a line of code, ask yourself: "Why was the old line wrong? Why is the new line correct?" If you can't answer, ask AI to explain.

Second, verify it addresses the root cause. Sometimes AI patches symptoms. If the real problem is that you're passing the wrong data type, but AI adds code to handle multiple data types, that's treating symptoms. Better to fix where you're passing the wrong type.

Third, check complexity. If AI's fix uses advanced features or convoluted logic, ask: "This works but seems complex. Can you solve it more simply using basic Python?"

Finally, test the fix. Don't just run it once and assume it works. Run your verification tests. Add new tests for the bug case. Make sure the fix doesn't break something else.

Red flags: If you see code you don't understand, stop. Ask AI to explain or simplify. Never have code in your project that you can't explain.
:::

# The Debug-Fix-Verify Cycle

1. **Bug appears** → verification failed
2. **Investigate** → understand what's wrong
3. **Hypothesize** → form theory about cause
4. **Test hypothesis** → use prints, AI explanations
5. **Implement fix** → simple, clear solution
6. **Verify** → run tests to confirm fix
7. **Prevent** → add test for this bug case

::: {.notes}
Let's put it all together into a complete debugging workflow.

Start when verification fails - a test doesn't pass, or output is wrong, or there's an error. This is your signal something's broken.

Investigate by reading the error, adding print statements, examining the code flow. Understand what's actually happening versus what should happen.

Form hypotheses about the cause. Use AI to help brainstorm possibilities if you're stuck.

Test your hypotheses systematically. Add prints, run the code, check values. Narrow down to the root cause.

Implement a fix - ideally the simplest fix that addresses the root cause. If you need AI's help, ask for simple solutions.

Verify the fix works. Run your original tests. Run the code that was broken. Make sure it's now correct.

Finally, prevent recurrence. Add a test case that specifically checks for this bug. If you fixed a divide-by-zero error, add a test that ensures your function properly handles zero input. This prevents regression - the bug coming back later.

This cycle is professional debugging. It's systematic, thorough, and ensures quality.
:::

# Debugging AI-Generated Code: Special Considerations

**Ask yourself:**
- Does this code match my specification?
- Is the logic sound, even if implementation is unfamiliar?
- Are there hidden assumptions in the AI's solution?
- Would I have solved it this way?

**If uncertain, ask AI:**
"Why did you implement it this way instead of [your approach]?"

::: {.notes}
Debugging AI-generated code has some unique aspects because you didn't write it yourself.

First question: Does this match what I asked for? Sometimes AI misunderstands your specification. Check that the code actually implements your requirements, not what AI thought you meant.

Second: Is the logic sound? AI might use unfamiliar patterns, but the underlying logic should make sense. If you can't follow the logic, that's a problem - either the code is unnecessarily complex, or you need to understand it better before debugging.

Third: Hidden assumptions. AI might assume certain input formats, or that data is pre-validated, or other conditions. These assumptions might not match your use case. Look for places where the code might break if assumptions don't hold.

Fourth: Different approaches. AI might solve the problem differently than you would. That's okay, as long as it's correct and reasonably simple. But understanding the difference helps you debug.

When uncertain, ask AI to explain its choices: "Why did you use a dictionary here instead of a list?" Understanding the reasoning helps you evaluate if it's appropriate for your needs.
:::

# Practical Debugging Exercise

**Scenario:** Function should find average of positive numbers only.

```python
def average_positive(numbers):
    total = sum(numbers)
    count = len(numbers)
    return total / count

# Should return 3.0, but returns 1.5
result = average_positive([2, -1, 4])
```

**Your debugging steps:**
1. What's wrong? Expected 3.0, got 1.5
2. Hypothesis? Including negative numbers
3. Verify? Add prints for total and count
4. Fix? Filter to positive numbers first
5. Test? Run with various inputs

::: {.notes}
Let's work through a concrete example. We have a function that should find the average of only positive numbers in a list, but it's returning the wrong result.

Step 1 - Identify the problem: Expected 3.0 (average of 2 and 4), got 1.5. That's wrong.

Step 2 - Form hypothesis: The function is including all numbers (2, -1, 4), giving total 5, count 3, average 1.67... wait, that's not even 1.5. Let me think... Oh, it might be including -1, which would make the total 5 and average 1.67. Actually, (2 + -1 + 4) / 3 = 5/3 = 1.67, not 1.5. Let me check my math. Oh wait, if it's (2 + 4) / 3 it's 2.0, not 1.5 either. 

Actually, add prints to see what's actually happening:

```python
def average_positive(numbers):
    print(f"Input: {numbers}")
    total = sum(numbers)
    print(f"Total: {total}")
    count = len(numbers)
    print(f"Count: {count}")
    return total / count
```

Run this and you see: Input: [2, -1, 4], Total: 5, Count: 3. So 5/3 ≈ 1.67.

Oh, I mis-stated the expected result in the example. The point stands - the function is using all numbers instead of filtering to positives.

Step 3 - Fix: Filter first:
```python
positives = [n for n in numbers if n > 0]
total = sum(positives)
count = len(positives)
```

Step 4 - Verify: Test with [2, -1, 4], should get (2+4)/2 = 3.0. Test with all negatives, all positives, mixed cases.

This is the debugging process in action.
:::

# Debugging Checklist

**Before asking for help (human or AI):**
- [ ] Read the error message carefully
- [ ] Add print statements at key points
- [ ] Form a hypothesis about the cause
- [ ] Check for common patterns (off-by-one, typos, scope)
- [ ] Verify your assumptions with prints

**When asking AI for help:**
- [ ] Describe expected vs actual behavior
- [ ] Include relevant code, not everything
- [ ] Ask for explanation, not just fixes
- [ ] Request simple, beginner-appropriate solutions

::: {.notes}
Here's a practical checklist for debugging. Use this every time you encounter a bug.

Before you ask anyone for help - whether a classmate, instructor, or AI - do these things first. This self-reliance builds your debugging skills.

Read the error message. Really read it. What does it say? Where did it happen? What's the chain of calls?

Add strategic print statements. See what's actually happening, not what you think is happening.

Form at least one hypothesis. "I think this is happening because..." This shows you're thinking systematically.

Check common patterns. Is it an off-by-one? A typo? A scope issue? These are quick to check.

Verify assumptions. You assume the variable contains a list of numbers? Print it and check.

When you do ask AI, be specific about the problem. Don't just dump all your code. Give the relevant section, describe what should happen versus what does happen.

Ask for understanding first. "Why might this happen?" before "How do I fix it?"

Request appropriate complexity. "Explain/fix this using only basic Python concepts."

This checklist makes you more effective at both independent debugging and collaborative debugging with AI.
:::

# Common Pitfalls in AI-Assisted Debugging

**Pitfall 1:** Asking AI to fix without understanding
- **Better:** Ask AI to explain, then fix yourself

**Pitfall 2:** Accepting complex fixes blindly
- **Better:** Request simpler solutions at your level

**Pitfall 3:** Not testing the AI's fix thoroughly
- **Better:** Run all tests, add edge cases

**Pitfall 4:** Fixing symptoms instead of root causes
- **Better:** Ask "why did this happen?" until you reach the real issue

::: {.notes}
Let's address common mistakes students make when debugging with AI assistance.

Biggest pitfall: Using AI as a magic fix button. "Here's my code, it's broken, fix it." This teaches you nothing and often results in fixes you don't understand. Always ask for explanation first, then attempt the fix yourself.

Second pitfall: Accepting complexity. AI might suggest using advanced features or libraries you haven't learned. Don't accept this. Ask for a fix using only concepts you know. This is not limiting - it's ensuring you can maintain the code.

Third pitfall: Minimal testing. AI suggests a fix, you run it once, it seems to work, you move on. But did you test edge cases? Did you run your full test suite? A fix that works for one input might fail for others.

Fourth pitfall: Surface-level fixes. AI patches the immediate error, but doesn't address why the error occurred. If AI adds a null check, ask yourself: why is the value null? Should it be? Is the null check masking a deeper problem?

Avoid these pitfalls by staying engaged in the debugging process. AI assists, but you think critically and make decisions.
:::

# Building Debugging Intuition

**With practice, you'll develop:**
- Pattern recognition (seen this error type before)
- Quick hypothesis formation (likely caused by...)
- Efficient investigation (check these places first)
- Appropriate AI use (ask for help here, handle this myself)

**How to practice:**
- Debug code intentionally broken by instructor
- Debug your own failed tests
- Debug AI-generated code you don't understand
- Explain bugs to others (rubber duck debugging)

::: {.notes}
Debugging is a skill that improves dramatically with practice. As you debug more code, you develop intuition.

Pattern recognition: You start recognizing "Oh, that's a type error" or "That looks like an off-by-one." Experience teaches you what errors tend to look like.

Quick hypothesis: Instead of being completely lost, you quickly think "This might be a scope issue" or "I bet the input type is wrong." You know what to check first.

Efficient investigation: You learn which print statements give the most useful information. You know which parts of the code to examine first for certain error types.

Appropriate AI use: You develop judgment about when you can figure it out yourself versus when AI's input would be valuable. You know what questions to ask AI.

To build this intuition, practice deliberately. When instructors give you intentionally broken code, treat it as practice. When your own tests fail, debug systematically. When AI generates code that seems wrong, investigate it.

Rubber duck debugging - explaining the problem out loud, even to an inanimate object - helps you think through it. Often you'll solve it while explaining it.

Every bug you debug makes you better at debugging the next one.
:::

# Debugging and Testing: Two Sides of Quality

**Testing:** Verifying code works (proactive)
**Debugging:** Investigating when it doesn't (reactive)

**The cycle:**
1. Write specification & tests (Module 1)
2. Implement (with AI assistance)
3. Tests fail → Debug (Module 2)
4. Fix and verify
5. Tests pass → Confidence in quality

::: {.notes}
Debugging and testing work together as complementary quality assurance activities.

Testing is proactive - you write tests that specify correct behavior before or shortly after implementation. These tests catch bugs early.

Debugging is reactive - when tests fail or unexpected behavior occurs, you investigate why.

The cycle works like this: You specify what you want with tests. You implement (perhaps with AI). Tests fail because there's a bug. You debug to find the bug. You fix it. Tests pass, giving you confidence the code works.

Good tests make debugging easier - they tell you exactly what's broken. Systematic debugging makes you write better tests - you learn what edge cases to test for.

Together, testing and debugging form your quality control process. Testing prevents bugs, debugging finds and fixes those that slip through. Both are essential skills for working effectively with AI-generated code.

In Module 3, we'll extend this to strategic test design - how to think comprehensively about testing larger systems. But the foundation is always the same: specify, implement, verify, debug, fix, verify again.
:::

# Key Takeaways

* **Debugging is investigation**, not random fixes
* **Understand the error** before attempting solutions
* **Use prints strategically** to see what's actually happening
* **Form and test hypotheses** systematically
* **AI explains, you fix** - maintain understanding
* **Verify fixes thoroughly** - run all tests
* **Learn from bugs** - add tests to prevent recurrence

::: {.notes}
Let's crystallize the key lessons from this module.

Debugging is detective work. You gather evidence, form theories, test them, and reach conclusions. This systematic approach is far more effective than trial-and-error.

Understanding errors is the first step to fixing them. Use AI to explain errors you don't understand, but don't skip to the fix.

Print statements are your window into code execution. Use them liberally to verify assumptions and trace program flow.

Hypothesis-driven debugging is professional debugging. Form theories about causes, design tests for those theories, draw conclusions. Iterate until you find the root cause.

AI is your research assistant - it explains, suggests hypotheses, helps you understand. But you're the detective who evaluates evidence and implements solutions. Maintain that understanding.

Thorough verification ensures your fix actually works. Don't just make the error go away - ensure the code now does what it should.

Finally, learn from every bug. Each one teaches you something. Add tests that would have caught this bug, so it can't return.

These practices make you an effective debugger whether working with your own code or AI-generated code.
:::

# Practice Challenge

**Debug this function:**
```python
def count_vowels(text):
    vowels = "aeiou"
    count = 0
    for letter in text:
        if letter in vowels:
            count += 1
    return count

# Should return 3 for "Hello", but returns 2
```

**Your process:**
1. What's the expected vs actual behavior?
2. Add prints to trace execution
3. Form hypothesis about the cause
4. Identify the root cause
5. Implement simple fix
6. Verify with multiple test cases

::: {.notes}
Here's your practice challenge. This function should count vowels but gives the wrong answer for "Hello".

Work through the debugging process:

First, clarify the problem. Expected: 3 (e, o, o). Actual: 2. 

Add prints inside the loop: print(f"Checking '{letter}', count is {count}"). Run it.

You'll see it checks 'H', 'e' (count becomes 1), 'l', 'l', 'o' (count becomes 2). It's only counting lowercase vowels!

Hypothesis: The 'H' is uppercase, and uppercase vowels aren't in the vowels string.

Root cause: Case sensitivity. The code only checks for lowercase vowels.

Fix: Convert to lowercase before checking:
```python
if letter.lower() in vowels:
```

Or include uppercase vowels in the string:
```python
vowels = "aeiouAEIOU"
```

Test with: "Hello", "HELLO", "aEiOu", "", "bcdfg"

This simple example demonstrates the complete debugging workflow. Practice this process and it becomes second nature.
:::
