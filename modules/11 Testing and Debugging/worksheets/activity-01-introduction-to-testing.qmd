---
title: "Mastering Python Tests with Doctest"
subtitle: "A Practical Approach to Building Reliable Software Through Documentation, Enhanced by AI"
format: jupyter-notebook
jupyter: python3
---

## Learning Objectives
By the end of this activity, you will be able to:
- Understand and apply basic principles of unit testing in Python using `doctest`.
- Develop simple tests embedded within function docstrings.
- **Understand how AI tools can specifically assist in creating, improving, and understanding `doctests` and other tests.**
- **Apply an ethical and effective workflow for AI-assisted testing.**
- See how tests also serve as clear documentation and examples.

## Introduction
Unit testing is crucial for ensuring your code works as expected. `doctest` offers a straightforward way to embed tests directly in your function's documentation (docstrings). This means your examples become live, verifiable tests. In the age of AI, these tools can further supercharge your ability to write effective `doctests` and understand broader testing principles, helping you catch errors early, simplify debugging, and maintain clean, reliable code.

## Using AI Tools Ethically & Effectively for Testing
AI wonâ€™t make testing obsolete, but it can be a powerful assistant. Your understanding and critical evaluation remain essential.

> **How AI Can Help You With `doctest` (and Beyond):**
>
> * **Generate `doctest` Examples Automatically:** You can ask AI tools to help generate `doctest` examples for your functions.
>     * **Example prompt:** "Write some `doctest` examples for this Python function, including common cases and an edge case: `[your function code]`"
>     * Good AI tools can help identify potential edge cases you might not have thought of for your docstring examples.
> * **Explain and Improve Existing `doctests`:** If you have `doctests` that are failing or you're not sure how to write a good example for a complex scenario, AI can help.
>     * **Example prompt:** "Can you explain why this `doctest` might be failing for my function?" or "Can you suggest a better way to write a `doctest` for this function to cover scenario X?"
>     * AI can review your `doctest` examples and suggest clearer ways to write them or more descriptive accompanying text in your docstring.
> * **Find Missing `doctest` Scenarios:** AI can help you think about what situations your current `doctests` might not cover.
>     * **Example prompt:** "Looking at this function and its current `doctests`, what other scenarios or edge cases should I consider adding `doctest` examples for?"
> * **Help Generate Example Data/Scenarios:** While `doctest` doesn't typically use complex mocks in the same way `unittest` or `pytest` do, AI can help you think through realistic input data or scenarios to use in your `doctest` examples.
>
> **Tips & Tricks for an AI-Assisted `doctest` Workflow:**
>
> * **Prompt Iteratively:** Start with a simple request (e.g., "generate basic `doctest` examples") and then refine. Ask for more examples, specific edge cases, or explanations.
> * **Use AI to Review Coverage (Conceptually):** After writing your `doctests`, ask an AI: "What types of inputs or logic paths in my function do these `doctest` examples not seem to cover?"
> * **Focus on behaviour:** Encourage AI to help you write `doctests` that validate what the code *should do* (the expected output for a given input).
> * **Human Oversight is Key:**
>     * **Review AI Suggestions:** Always critically review any `doctest` examples or explanations provided by AI. Ensure they make sense, correctly represent the function's intended behaviour, and truly test what you want to test.
>     * **Understand, Don't Just Copy:** The goal is for *you* to learn and understand. If an AI provides a `doctest`, make sure you understand why it's written that way and why it's relevant.
>
> **Why Write Tests (like `doctests`) in the AI Era?**
>
> Can't we just use AI to fix bugs on demand? Hereâ€™s why thatâ€™s risky and why tests remain vital:
>
> | Without Tests (or `doctests`)                     | With Tests (like `doctests`)                                  |
> | :------------------------------------------------ | :------------------------------------------------------------ |
> | You (or users) discover bugs late.                | You catch many bugs before they cause bigger problems.        |
> | Changing code (refactoring) is risky.             | You can refactor confidently; `doctests` verify changes.    |
> | AI fixes one bug but might introduce another.     | AI-assisted code changes can be validated by running tests. |
> | No safety net if changes break something.         | Fast feedback loop from running tests.                        |
> | Code's intended use can be unclear.               | `doctests` serve as clear, executable documentation.       |
>
> `doctests` (and other tests) are your documentation, your validation, and your safety net. AI can make writing and understanding them easier and faster.
>
> **Bottom Line:**
> AI supercharges your ability to test, but it doesnâ€™t replace the need for well-thought-out, maintainable test code that *you* understand. Your critical thinking in designing, reviewing, and understanding tests is irreplaceable.

## Key Concepts
- **Unit Testing:** Testing individual components of the software independently for correct operation.
- **`doctest` Module:** A Python module that searches for pieces of text that look like interactive Python sessions in docstrings, and then executes those sessions to verify that they work exactly as shown.
- **Test Case (in `doctest`):** An example written directly in a docstring, showing an input to a function and its expected output.
- **Assertions (in `doctest`):** Implicit. A `doctest` passes if the output produced by running the example code exactly matches the output written in the docstring.

## Application Activities

### Activity 1: Writing Your First Test with `doctest`
**Scenario:** You have a Python function `add(a, b)` that returns the sum of two numbers. Write `doctest` examples within its docstring to verify its correctness.

1.  Define the function `add(a, b)`.
2.  Inside its docstring, write examples for:
    * `add(2, 3)` should result in `5`
    * `add(-1, 1)` should result in `0`
3.  Import the `doctest` module and run the tests.

*Your response:*
```{python}
import doctest

def add(a, b):
    """
    Adds two numbers and returns the result.

    Examples:
    >>> add(2, 3)
    5
    >>> add(-1, 1)
    0
    >>> add(10, -5)
    5
    """
    return a + b

if __name__ == '__main__':
    doctest.testmod(verbose=True)
```

::: {.callout-tip}
ðŸ’¡ **AI Tip**: Stuck on what `doctest` examples to write? Provide your `add` function to an AI and ask: "Suggest three useful `doctest` examples for this Python function, including one that tests negative numbers." Critically review its suggestions.
:::

### Activity 2: Analyse, Test, and Improve with `doctest`
**Scenario:** You are given a function `multiply(a, b)` that *should* multiply two numbers but contains a bug. Write `doctests` that reveal the bug, then fix the function so the tests pass.

1.  Define the buggy `multiply` function.
2.  Write `doctests` that would pass if the function were correct.
3.  Run the `doctests` to see the failure.
4.  Identify the bug based on the failed test and correct the function.
5.  Re-run the `doctests` to confirm they now pass.

*Your response:*
```{python}
import doctest

# Buggy function
def multiply_buggy(a, b):
    """
    This function is supposed to multiply two numbers.
    Examples:
    >>> multiply_buggy(3, 5)
    15
    >>> multiply_buggy(-2, 4)
    -8
    """
    return a + b  # This is the incorrect operation

# Corrected function
def multiply(a, b):
    """
    Multiplies two numbers and returns the result.
    Examples:
    >>> multiply(3, 5)
    15
    >>> multiply(2, 0)
    0
    >>> multiply(-2, 4)
    -8
    >>> multiply(0.5, 4)
    2.0
    """
    return a * b

if __name__ == '__main__':
    print("Testing buggy function (expect failures):")
    # To test only specific doctests, you might need more advanced usage or run them in separate contexts.
    # For this activity, focus on correcting 'multiply' and testing the whole module.
    # If multiply_buggy were the only one, its doctests would show errors.
    # We'll assume we're focusing on getting 'multiply' right.
    doctest.testmod(verbose=True)
```

::: {.callout-tip}
ðŸ’¡ **AI Tip**: If a `doctest` for `multiply_buggy` fails, copy the function and the failing `doctest` example into an AI tool. Ask: "This `doctest` failed for my function. Can you explain what the function is doing versus what the doctest expects, and suggest what kind of logical error might cause this?"
:::

## Extension Challenge
Write a function `subtract(a, b)` that subtracts the second number from the first. Include comprehensive `doctests` in its docstring to cover various cases.
* **AI Brainstorming:** Before you write them, ask an AI: "What are some important edge cases and common scenarios I should consider when writing `doctest` examples for a Python subtraction function?" Use these ideas to create a robust set of `doctests`.

*Example of how to start the function:*
```{python}
import doctest

def subtract(a, b):
    """
    Subtracts b from a.
    (Add your doctests here based on AI brainstorming and your own ideas)
    Examples:
    >>> subtract(10, 3)
    7
    """
    return a - b

if __name__ == '__main__':
    doctest.testmod(verbose=True)
```

## Reflection
Reflect on how you have used this knowledge:
- How might `doctest` help you write clearer function documentation that is also verifiable?
- How can the AI-assisted testing strategies discussed improve the quality and coverage of your `doctests`?
- What are the benefits of writing tests *before* or *as* you write your code, even with AI tools available to help generate them later?
- How did you ensure that you understood any AI suggestions for `doctests` rather than just accepting them?

Remember, the goal is to integrate testing into your regular programming practices. `doctest` provides an accessible way to do this, and AI can help you be more effective and thorough, as long as you remain the critical thinker in charge.



## Beyond Doctest: A Glimpse into Advanced Testing Concepts

For students interested in exploring testing further, here are some concepts that become important as software projects grow in complexity. These are typically associated with more comprehensive testing frameworks like `unittest` or `pytest`, but understanding the principles is broadly useful.

### Advanced Discussion: Understanding Test Coverage

**What is Test Coverage?**

Test coverage is a metric used in software testing that measures the extent to which your codebase is executed when your test suite runs. It essentially tells you which lines of your code, branches, or functions have been "touched" by your tests. The result is usually expressed as a percentage.

**Why is Test Coverage Important?**

* **Identifying Untested Code:** The primary benefit is that it highlights parts of your application that are *not* currently being tested. These untested areas could harbor hidden bugs.
* **Confidence in Refactoring:** If you have high test coverage, you can refactor (restructure or improve) your code with greater confidence. If your changes inadvertently break existing functionality, well-written tests covering that code should fail, alerting you to the issue.
* **Guiding Test Creation:** Coverage reports can guide you in writing new tests to cover previously untested areas, making your test suite more comprehensive.
* **Maintaining Quality Over Time:** As new features are added or code is modified, tracking coverage can help ensure that testing efforts keep pace.

**Types of Coverage (High-Level):**

There are several ways to measure coverage, including:

* **Statement Coverage:** Did each statement in the program execute?
* **Branch Coverage:** For every `if/else` or other control structure, did every possible branch (e.g., both the `if` and the `else` path) get executed?
* **Function/Method Coverage:** Did each function or method in the program get called?

Branch coverage is generally considered more thorough than statement coverage because a statement might be executed but only under one condition of a branch.

**How is Coverage Measured?**

Specialised tools are used to measure test coverage. For Python, a very popular tool is `coverage.py`, which can be used with frameworks like `pytest` or `unittest`. These tools typically:
1.  Run your test suite.
2.  Monitor which parts of your application code are executed during the tests.
3.  Generate a report detailing the coverage, often highlighting specific lines or sections that weren't covered.

While `doctest` itself is excellent for example-based testing and documentation, it doesn't natively offer the same kind of sophisticated coverage reporting as these dedicated tools and frameworks. Achieving detailed coverage metrics usually involves integrating with these more comprehensive systems.

**Important Considerations & Caveats:**

* **100% Coverage is Not a Silver Bullet:** Achieving 100% test coverage does not guarantee your code is bug-free. You could have 100% coverage with tests that don't actually assert the correct behaviour, or you might be missing tests for certain logical conditions even if all lines are hit.
* **Quality Over Quantity:** The quality of your tests is far more important than just the coverage percentage. A few well-written tests that check critical functionality and edge cases are better than many trivial tests that achieve high coverage but don't meaningfully validate the code.
* **Context Matters:** The ideal level of coverage can depend on the criticality of the software module. Mission-critical systems might aim for very high coverage, while other less critical parts might have different targets.

**Test Coverage in the AI Era:**

AI tools can also play a role in relation to test coverage:
* **Identifying Gaps:** AI might be able to analyse code and existing tests to suggest areas with low coverage or specific paths that are untested.
* **Suggesting Tests for Coverage:** You could potentially ask an AI to help generate tests aimed at covering specific lines or branches identified by a coverage tool.
* **The Same Caveats Apply:** Even if AI helps generate tests to improve coverage, human oversight is crucial to ensure these tests are meaningful, assert the correct behaviour, and don't just blindly "hit lines" without purpose. The goal is robust, well-understood code, not just a high percentage.

**In Summary:**

Understanding test coverage is valuable as you progress in your software development journey. It's a tool that, when used wisely, can significantly improve the quality and reliability of your software by helping you understand how thoroughly your tests exercise your code. While `doctest` serves a fantastic purpose for clear, example-based testing within documentation, coverage analysis typically involves dedicated tools often used with more extensive testing frameworks.

---
