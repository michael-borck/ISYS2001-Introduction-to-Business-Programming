{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb20a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Simple example of fetching a weather page\n",
    "url = \"https://example.com/weather\"\n",
    "response = requests.get(url)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "print(f\"Content: {response.text[:100]}...\")  # Print first 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adb941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_robots_txt(url):\n",
    "    robots_url = f\"{url}/robots.txt\"\n",
    "    response = requests.get(robots_url)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Robots.txt content:\\n{response.text}\")\n",
    "    else:\n",
    "        print(f\"No robots.txt found at {robots_url}\")\n",
    "\n",
    "check_robots_txt(\"https://curtin.edu.au\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2dd945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "url = \"https://example.com/weather?city=New York&units=metric\"\n",
    "parsed_url = urlparse(url)\n",
    "query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "print(f\"Scheme: {parsed_url.scheme}\")\n",
    "print(f\"Domain: {parsed_url.netloc}\")\n",
    "print(f\"Path: {parsed_url.path}\")\n",
    "print(f\"Query parameters: {query_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e60d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://wttr.in/perth\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the weather page\")\n",
    "    print(f\"Content: {response.text[:200]}...\")  # Print first 200 characters\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59439593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <h1>Weather Forecast</h1>\n",
    "    <p class=\"temperature\">25°C</p>\n",
    "    <p class=\"condition\">Sunny</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "temperature = soup.find('p', class_='temperature').text\n",
    "condition = soup.find('p', class_='condition').text\n",
    "\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Condition: {condition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37200753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://curtin.edu.au\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    href = link.get('href')\n",
    "    text = link.text\n",
    "    print(f\"Link: {text} -> {href}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946eed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = \"\"\"\n",
    "<table id=\"weather-forecast\">\n",
    "  <tr>\n",
    "    <th>Day</th>\n",
    "    <th>Temperature</th>\n",
    "    <th>Condition</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Monday</td>\n",
    "    <td>25°C</td>\n",
    "    <td>Sunny</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Tuesday</td>\n",
    "    <td>22°C</td>\n",
    "    <td>Cloudy</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "table = soup.find('table', id='weather-forecast')\n",
    "\n",
    "for row in table.find_all('tr')[1:]:  # Skip header row\n",
    "    columns = row.find_all('td')\n",
    "    day = columns[0].text\n",
    "    temp = columns[1].text\n",
    "    condition = columns[2].text\n",
    "    print(f\"{day}: {temp}, {condition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efdd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Global_surface_temperature#Global_temperature_record\"\n",
    "tables = pd.read_html(url)\n",
    "weather_df = tables[0]  # Assuming the weather table is the first table on the page\n",
    "print(weather_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1f28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we have a cleaned DataFrame 'weather_df'\n",
    "weather_df.to_csv('weather_data.csv', index=False)\n",
    "print(\"Data saved to weather_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def fetch_with_delay(url, delay=1):\n",
    "    response = requests.get(url)\n",
    "    print(f\"Fetched {url}: Status {response.status_code}\")\n",
    "    time.sleep(delay)  # Wait for 1 second before next request\n",
    "    return response\n",
    "\n",
    "# Usage\n",
    "urls = [\"https://example.com/weather/day1\", \"https://example.com/weather/day2\"]\n",
    "for url in urls:\n",
    "    fetch_with_delay(url)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
